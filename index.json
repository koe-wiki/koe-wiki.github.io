[
{
	"uri": "http://koe-wiki.github.io/features/time-domain/",
	"title": "Frequency domain features",
	"tags": [],
	"description": "",
	"content": "Spectral Flux Is the measure of the changes in the shape of the spectrum over time. SF is computed as the Euclidean ($L^2$) distance of two consecutive spectral frames with normalised PSD\nSpectral Slope For many natural audio signals, the energy of the spectrum tends to decrease towards the higher end of the frequency range, the \u0026ldquo;slope\u0026rdquo; of which is related to the nature of the sound source. This parameter can be found via linear regression of the magnitude: $$x(f) = slope \\times f + intercept$$ where $x(f)$ is the magnitude function and $f$ is the frequency value. In addition to slope, the intercept and regression error can also be used as features.\nSpectral Centroid is the centre of gravity of the spectrum, calculated as the weighted mean of the frequencies using their normalised magnitude as weights:\n$$SC = \\sum_{n=1}^{N} \\Big(f(n) \\times x(n)\\Big) \\Bigg/ \\sum_{n=1}^{N} x(n)$$\nWhere $N$ is the total number of frequency bins, $a(n)$ is the normalised magnitude of the spectrum at bin $n$ and $f(n)$ is the centre frequency of bin $n$. Value of SC is the frequency where most of the energy is concentrated in, which is usually where the dominant frequency is found.\nSpectral Centre Is the frequency where half of the energy in the magnitude spectrum is above and half is below\nSpectral Rolloff Is defined as the \\textit{N}% percentile of the power spectrum. In practice, the value of \\textit{N} is usually at the higher end (85, 95, etc).\nSpectral Centre is a Spectral Rolloff with $N=50$\nSpectral Flatness Also known as Wiener entropy is a measure of the noisiness of the spectrum, of which the higher value correspond to a more uniform distribution of power over all frequencies. It is computed as the ratio of the geometric mean over the arithmetic mean of the spectrum:\n$$SF = \\sqrt[N]{\\prod_{n=1}^{N}x(n)} \\Bigg/% \\frac{1}{N} \\sum_{n=1}^{N}x(n)$$\nSpectral Crest Is the opposite of Spectral Flatness. it measures how peaky the spectrum is. A noisy spectrum have low value of SCF compare to a clear tonal spectrum. It is calculated as the ratio of the peak over the mean frequency value:\n\\begin{equation*} SCF = \\max_{1 \\le n \\le N} x(n) \\Bigg/ \\frac{1}{N} \\sum_{n=1}^{N}x(n) \\end{equation*}\nBandwidth Also known as Spectral Spread, is usually defined as the magnitude-weighted average of the differences between the spectral components and the spectral centroid\n\\begin{equation*} BW = \\sum_{n=1}^{N} \\Big(\\big(f(n)-SC\\big) \\times x(n)\\Big) \\Bigg/ \\sum_{n=1}^{N} x(n) \\label{eq:bandwidth} \\end{equation*}\nFundamental Frequency Is the lowest frequency of a harmonic series. It concurs with the rate of oscillation of the sound source. The FF is the same as the dominant frequency in case of pure-tone signals, which makes it relatively straightforward to determine. However, this is not the case with speech and many bird\u0026rsquo;s vocalisations, which produce complex sound (having harmonic structure). The fundamental frequency of a harmonic series can be determined as either the lowest harmonic or more reliably (in the case of missing fundamental) by finding the largest common denominator of the harmonics.\nHarmonic Ratio Is the ratio of harmonic power to total power, it is computed in the autocorrelation domain and defined as the maximum value of the normalised autocorrelation function (after having ignored the zero-lag peak) within a frame of length $k$:\n\\begin{equation*} HR = \\max_k \\Bigg(\\sum\\nolimits_j \\Big(s(j)\\times s(j-k)\\Big) \\Big/ \\sqrt{\\sum\\nolimits_j s(j)^2 \\sum\\nolimits_j s(j-k)^2} \\Bigg) \\end{equation*}\nWhere $s(j)$ is the function of sound pressure in time domain.\n"
},
{
	"uri": "http://koe-wiki.github.io/user-manual/",
	"title": "User Manual",
	"tags": [],
	"description": "",
	"content": "Koe is open-source web-based software for analysing animal vocalisations.\nRead the Methods in Ecology and Evolution paper here. Please cite this paper if you publish using Koe\nIt features tools for visualising, segmenting, measuring, classifying, data-filtering and exporting acoustic units, and analysing sequence structure (syntax). It is an end-to-end acoustic database solution, especially suitable for animal species with distinct acoustic units.\nYou can use it on any device at koe.io.ac.nz, which makes it ideal for collaboration, education, and citizen science.\nFor a quick overview and demonstration of program features, see this video:\n  Click here to download NZ bellbird song recordings to follow along with the user manual\nTable of Contents  Why use Koe? Overview of Koe workflow Navigating Koe Create an account Create a new database Add collaborators to a database and set permission levels Set database restore points Upload songs Upload raw recordings and divide into songs Recordings too big to upload? Use this tool to automatically split your wav files Segment songs into units, or\u0026hellip; Alternatively, import unit segmentation from csv Extract unit features Construct ordination View interactive ordination and classify units Calculate similarity View interactive unit table and classify units  Sorting by columns Classifying   Exemplars Filtering data View all songs Mine sequence structure Visualise sequence structure as a network Exporting your data Copy songs to another database Make database subsets (collections) Keyboard shortcuts [[Analysis Tutorial: Case Study of NZ bellbird song]] [[Frequently Asked Questions]]  Table of contents generated with markdown-toc\nWhy use Koe? Back to table of contents\nAcoustic communication is fundamental to the behaviour of many species. If we want to understand animal behaviour we need tools that allow us to objectively examine the details of vocalisations. What information are they sharing, and how is it encoded?\nOften, acoustic communication is structured as a temporal sequence of distinct acoustic units (e.g. syllables), where information is encoded in the types of units and sometimes their temporal arrangement (syntax). In such cases, this is the flowchart for acoustic analysis:\nClassification is a key step, because once you have a dataset of labelled units, you can analyse repertoire size and sequence structure and compare between individuals, sexes, sites, and seasons.\nManual classification by human eye and ear remains the primary and most reliable method for most species, but is hindered by a lack of tools, especially for large and diverse datasets.\nThat’s where Koe comes in. By facilitating large-scale, high-resolution classification and analysis of acoustic units, Koe opens up many possibilities for bioacoustics research.\nOverview of Koe workflow Back to table of contents\nWe designed the Koe workflow to be intuitive and flexible. Here is a suggested workflow:\n  Import raw recordings and divide into “songs” (vocalisation bouts), then segment songs into their constituent acoustic units.\n  You can extract acoustic features from units. These features are used to calculate unit similarity and expedite classification in two ways: through interactive ordination, and similarity indices.\n  Interactive ordination is a major time saver for classification. The user can encircle groups of points on the ordination to see spectrograms, hear playback, and bulk-label groups of units. Harnessing the interactive ordination in conjunction with human audio-visual perception, this technique offers a major advance in both speed and robustness over existing acoustic classification methods.\n  Units can also be viewed as an interactive unit table. A key feature of the unit table is the similarity index, which lets you sort by acoustic similarity. Because similar-sounding units are grouped together, units can easily be selected in bulk and classified.\n  A catalogue of class exemplars is generated automatically, displaying up to 10 randomly chosen exemplars for each class, which serves as a useful reference during classification.\n  Koe gives you full control of your databases, allowing you to add collaborators and set permission levels. This makes it straightforward to conduct a classification validation experiment: grant judges labelling access to your database, and once they have independently classified the units, compile their labels to examine concordance.\n  Once units have been classified, songs can be visualised as sequences of unit labels; you can filter by sequence to identify all instances of a specific song type, for example. You can also mine sequence structure in detail with association rule algorithms and network visualisations.\n  Export data from any program view as csv.\n  Navigating Koe Back to table of contents\nIn Koe, on the left of the screen is a sidebar with program controls. At the top of the sidebar are User account controls for logging out / switching user; beneath that are controls specific to the active view. And beneath that is the workflow navigation pane with buttons to access different program views, labelled according to their function.\nCreate an account Back to table of contents\nGo to koe.io.ac.nz and click on Register to register an account.\nChoose a username, input your name and email address for purposes of Koe support correspondence, and choose a password. (We will never share your information with anyone).\nCreate a new database Back to table of contents\nUnder Manage your database, click New database to create a new database.\nAdd collaborators to a database and set permission levels Back to table of contents\nYou can add collaborators at any stage.\n  Click on Manage your database.\n  Select the round checkbox of the database you want to add collaborators to.\n  Click Add collaborator and Type the Koe username or Koe account email address of your intended collaborator.\n  Once added, you can set their permission level by double-clicking the Permission cell next to their username.\n  From lowest to highest permission level: View, Annotate, Import data, Copy files, Add files, Modify segmentation, Delete files, Assign user. Each subsequent permission level extends the permissions of the previous level as per the table below:\n   Level Permissions     View User can view data   Annotate User can view and annotate data   Import data User can import, view and annotate data   Copy files User can copy files, import, view and annotate data   Add files User can add files, copy files, import, view and annotate data   Modify segmentation User can modify segmentation, add files, copy files, import, view and annotate data   Delete files User can delete files, modify segmentation, add files, copy files, import, view and annotate data   Assign user User can assign additional users, delete files, modify segmentation, add files, copy files, import, view and annotate data    Set database restore points Back to table of contents\nChanges are saved to the database as soon as they happen. If you want to be able to revert to a previous state you will want to save database restore points.\n  Click on Manage your database.\n  Select the round checkbox of the database you want to set a restore point for.\n  Click Quick save or Full save at the bottom of the window to create a restore point. Quick save saves label data associated with each unit. Full save saves both label data and segmentation start/end points.\n  Please note that restore points do not affect which songs are in the database or which units are segmented. For example, if you make a save and then delete songs, reverting will not restore the songs to the database. Similarly, if you add songs, reverting will not remove the added songs. In the same way, if you add/delete units, reverting will not remove/restore the segmentation. What reverting will do is restore the label data and (for a full save) the segmentation start/end points for currently existing units. If you want to preserve all aspects of a database at a certain point, like a traditional \u0026lsquo;Save As\u0026rsquo;, then copy everything to a new database, as described in Copy songs to another database below.\nUpload songs Back to table of contents\nClick this link to download NZ bellbird songs to try out Upload songs feature and subsequent steps\nIf you have already-divided song files on your computer, you can upload these directly. On the navigation pane, click Upload songs and select up to 100 WAV files at once to upload.\nFor raw recordings that need to be divided into songs, see next step.\nUpload raw recordings and divide into songs Back to table of contents\nClick this link to download raw (unsplit) recordings of bellbird song to try Upload \u0026amp; split raw recordings\nTo upload a raw recording and divide it into song selections, do the following:\nOn the navigation pane, click Upload \u0026amp; split raw recording\n  In the Upload raw recording dialogue box, Click Upload (WAV only)\n  Select the recording you want to upload\n  Click Open\n  After selecting a file to upload, an Upload raw recording dialogue appears with Track name and Record date fields. The track name is automatically filled in based on the wav filename. For example, Raw_Recording_001.wav will by default appear as Raw_Recording_001.\n Click in the Track name field to modify the track name, if you wish.\n  Click in the Record date field to modify the date. By default, the record date is set as the date of recording upload to Koe.\n  Click Submit track info.\n   For recordings with more than one channel, select the desired channel.\n  Play the recording with the playback controls, looking for songs to segment. To play from the beginning, click Play button.\n  You can also play from a specific timepoint by clicking that point, then clicking Play.\n  With the controls beneath the spectrogram, you can:\n Adjust spectrogram zoom.\n  Adjust spectrogram colour map.\n  Adjust playback speed.\n  Adjust spectrogram contrast.\n  To create a song selection simply drag over the spectrogram to create a selection box.  Click the selection box to play it. Adjust the selection box endpoints by holding Shift and dragging the box handles that appear.\nFor each selection you make, a row appears in the table beneath.\nFill in annotation columns as desired by double-clicking in those cells.   You can set an automatic naming scheme for segmented songs. Click the Naming pattern box to create a naming scheme with any combination of the following components: Track name, Year, Month, Day, Order. You can also add custom components to the scheme, such as text strings, if you wish. Click Rename all to apply the naming scheme to all existing song selections.\n  Once you’ve finished making song selections, click Save to upload the songs to the database. The raw recording will not be saved, only songs – so make sure you finish partitioning all songs in one go.\n  Recordings too big to upload? Use this tool to automatically split your wav files Some bioacoustics researchers work with looong recordings (e.g. passive acoustic monitoring). Very large files can be cumbersome. But don\u0026rsquo;t worry! Yukio Fukuzawa has heard your cries and has come galloping to our rescue with a simple tool for automatically splitting large wav files into smaller wav files. You specify how long you want the audio sections to be, and how many seconds of overlap between sections.\nTo run the tool you will need to install Python. After that it\u0026rsquo;s just a simple line of code that you type into your command prompt.\nThe tool and instructions are found here: https://github.com/fzyukio/split-songs\nSegment songs into units Back to table of contents\nThis section describes how to segment songs into acoustic units. However, if you have already segmented units using other software, jump to the next section\nOn the navigation pane, click View all songs. You will see your list of songs; click on a song filename to open the song.\nMuch like the previous program view, you can play the song and adjust spectrogram appearance and playback speed. Drag over units on the spectrogram to segment. Mouse over a selection box to highlight that unit’s row in the table. Click a selection box for playback. Fill in annotation columns as desired. Then click Save segmentation to save to the database.\nYou can return to this view to alter segmentation at any time.\nNote: a Koe user interested in comparing entire songs as units for analysis can still use the software by selecting songs rather than syllables as units and classifying them using interactive ordination / unit tables. In a future update, we hope to offer both unit- and song-level comparisons simultaneously, so that a user does not have to choose.\nImport unit segmentation from csv Back to table of contents\nIf you have already segmented units using other software, you can import the unit segmentation start/end point data into Koe, and avoid doing it over again in Koe. You\u0026rsquo;re welcome!\n  Upload all the song wav files that you have start/end-point data for (see Upload songs)\n  Navigate to Classify and manage units \u0026gt; Unit table on the left-hand pane.\n  Click the Export current table as csv button.\n  Open the resulting csv file (in Excel, for example).\n  Leaving the column headers unchanged, input your desired filename and unit start/end time information into the table, with one row per unit.\n  (Note that start/end times are measured in ms from the start of the file. E.g. a unit with start column value of 680 and end column value of 750 means the unit begins 680 ms and ends at 750 ms from the start of the file. Koe will use this information to construct syllable boundaries.)\n Excel may have reformatted values in the date column. You need to make sure the date format is yyyy-mm-dd.\n  Save the updated csv file.\n  In Koe, go to Classify and manage units \u0026gt; Unit table, and click Import data (csv) to table. Browse to select your csv file.\n  Your data should appear, and if everything worked properly you will see spectrograms of segmented units.\nExtract unit features Back to table of contents\nYou can extract a wide array of acoustic features from units. These features are used to calculate unit similarity, which is used for constructing ordination and calculating similarity indices. You can also export extracted features as a data matrix (csv file).\nGo to Extract features and compare \u0026gt; Extract unit features, then:\n  Select the desired set of features (if in doubt, tick all of them to start with).\n  Select the desired set of aggregation methods (see below for explanation).\n  Type a name for the data matrix (e.g. “All features”).\n  Click Submit.\n  The Koe server will process your request and send a notification email when your extraction job finishes. (Typically this will take only a minute or two). You can see what sets of features you have previously extracted under the Existing data matrices dropdown in this view.\nHere are the features offered, with descriptions of each (coming soon):\n   Feature Description     spectral flatness    spectral bandwidth    spectral centroid    spectral contrast    tonnetz    spectral rolloff    chroma stft    chroma cqt    chroma cens    mfcc    zero crossing rate    total energy    aggregate entropy    average entropy    average power    max power    max frequency    frequency modulation    amplitude modulation    goodness of pitch    amplitude    normalise    entropy    mean frequency    spectral continuity    duration    frame entropy    average frame power    max frame power    dominant frequency     Most of these features are frame-based, with the result that units of different lengths are represented by unequal-length feature sequences. It is therefore necessary to aggregate these sequences to arrive at the same dimensional representation for each unit, so that units can be compared. Aggregation methods offered:\n   Aggregation method Description     Mean    Median    Standard Deviation    Divcon_3_mean    Divcon_5_mean    Divcon_7_mean     To export extracted feature matrices as csv for external use, click Download.\nIf your dataset is very big, you will need a lot of free RAM for this to work. For example, with 21k units, you will need at least 8 GB of free RAM.\nConstruct ordination Back to table of contents\nKoe can produce interactive two- or three-dimensional ordination plots, based on the unit features extracted in the previous step. Notably, two-dimensional plots can be used to classify units directly on the plot, which greatly expedites the classification process (see next section). To construct an ordination, go to Extract features and compare \u0026gt; Construct ordination and choose a data matrix from the Existing data matrices dropdown list (1).\nChoose an ordination method (2): PCA (Principal Components Analysis), ICA (Independent Component Analysis) or t-SNE (t-distributed Stochastic Neighbor Embedding) with PCA prepocessing. Since t-SNE preserves local structure in the data, it is particularly effective for defining and discriminating between different clusters.\nNote: t-SNE is controlled by two parameters, \u0026lsquo;perplexity\u0026rsquo;, which can be set to any value between 5 and 50, and number of iterations. By default, perplexity is set to 10 and number of iterations (n_iter) is set to 4000. There is no \u0026lsquo;correct\u0026rsquo; value of perplexity \u0026ndash; you must experiment with different values to achieve best results. If the perplexity value you choose is too high or low, you will find points on the plot do not cluster well by similarity, and will form convoluted banding patterns. To change the value, type, e.g. perplexity=30 in the Extra parameters box. The numbers of iterations should be adequate in most cases at 4000, but you can change this by typing, e.g. n_iter=5000 in the Extra parameters box. To control both parameters, separate with a comma as in perplexity=30, n_iter=5000.\nChoose (3) the desired number of dimensions: two or three. Then click Submit (4). The Koe server will process your request and send a notification email when your ordination construction job finishes. (Typically this will take only a few minutes).\nFor more tips on optimising your t-SNE ordination in Koe, see this FAQ\nView interactive ordination and classify units Back to table of contents\nClick Classify \u0026amp; manage units \u0026gt; Ordination. If you have constructed several ordinations, choose the one you want from the Current ordination dropdown list in the active view controls on the side bar. For 2D ordinations, a plot will be generated like the below.\nThe plot is controlled by a toolbar at the top left of the plot:\nWith the lasso tool (selected by default), encircle groups of points on the plot to see their spectrograms. Mousing over a point in a selection highlights the corresponding spectrogram in the left-hand panel. Click a point on the plot or its spectrogram for audio.\nClassify selections in bulk by clicking Label beneath the spectrograms pane. You can label at up to three levels of granularity.\nAs you add more classes, they each acquire a distinctive point type. Toggle class visibility by clicking the legend, zoom the plot (with the zoom tool) for more refined detail.\nYou can also view selections as an interactive unit table by clicking View table. More information on the unit table below.\nCalculate similarity Back to table of contents\nA second method for expediting classification is to use unit similarity indices. In the unit table (below) the similarity index is used to sort your units by spectral similarity, assisting rapid manual classification by grouping syllables in clusters that can be labelled in bulk.\nYou have two input options for calculating a similarity index: one is to use an extracted feature set, and the other is to use the coordinates of a constructed ordination. Go to Extract features and compare \u0026gt; Calculate similarity and select an option from Existing data matrices (for the former option) or Existing ordinations (for the latter). For optimal results we recommend using an existing t-SNE ordination as the input for the similarity index. Then click Submit. The Koe server will process your request and send a notification email when your similarity calculation job finishes. (Typically this will take only a few minutes).\nView interactive unit table and classify units Back to table of contents\nClick on Classify \u0026amp; manage units \u0026gt; Unit table to see all your units as an interactive unit table.\nMuch like in Excel, you can click on a cell to select it, or move around the grid with the arrow keys. Page Up and Page Down will page through one screen-height at a time. To hear a syllable, simply click on its spectrogram, or press spacebar with spectrogram cell selected. There is a speed slider at the top of the sidebar for slowing down playback; this helps our human ears hear details not apparent at full speed. Single-click on a cell to select cell contents (e.g. to copy to the clipboard), or double-click on a cell to alter cell content, if it is editable (editable cells contain a pencil icon).\nUnit table columns (re-order columns simply by dragging their headers):\n   Column name Description     ID Unique syllable ID   Start Start time of the syllable (milliseconds from song start).   End End time of the syllable (milliseconds from song start).   Duration Duration of the syllable (in milliseconds)   Individual The ID of the vocalising individual   Song Name of the song file the syllable comes from   Checkbox This is the column you will use for selecting rows.   Spectrogram Shows a spectrogram of the raw signal. Click to hear audio.   Label columns (Label, Subfamily, Family) These are where you label your units. There are three independent columns for different levels of label granularity. Label is for the most fine-scale labelling, Subfamily is broader, and Family is the broadest.   Sex The sex of the vocalising individual   Quality The quality of the song. For example, you could use EX = Excellent, VG = Very Good, G = Good, OK = Ok, B = Bad.   Similarity Index Sort by this column to group syllables by acoustic similarity (provided you have calculated similarity index as above).   Note Write notes about a row (free format)   Date The date of the raw recording   Added The date the song was added to the database    If you change \u0026lsquo;sex\u0026rsquo; or \u0026lsquo;species\u0026rsquo; column info associated with an individual animal, the changes will be reflected for all rows in the table belonging to that individual (you may need to refresh your browser to see the change propagate). This prevents mistakes, such as the same individual being ascribed male sex in some rows and female sex in other rows. It also means you only have to enter sex and species information for an individual in one row.\nA future Koe update will allow a user to add custom annotation columns, such as a species column. In the meantime a user can simply co-opt one of the existing annotation columns for their purposes, such as labelling species.\nSorting by columns Back to table of contents\nSort by a column by clicking on its header (e.g. Duration). To sort by multiple columns at once, Shift+click subsequent headers.\nClassifying Back to table of contents\nUnits can have three sets of labels at once, using the three label columns: Label, Subfamily and Family. You can use these to label at different scales, e.g. fine-scale, intermediate-scale and broad-scale classification. Or you may choose to use these columns for other purposes.\nSelect multiple rows of the same type by clicking their checkboxes or pressing spacebar with the checkbox cell selected. When you have finished selecting rows, press Ctrl+Shift+L to bulk label all selected rows at the fine-scale level. Ctrl+Shift+F bulk labels at the broad-scale “Family” level, and Ctrl+Shift+S bulk labels at the intermediate “Subfamily” level.\nAlternatively, click the menu button on the right-hand side of the Label, Subfamily or Family column header and select Bulk set value:\nA dialogue box will appear:\nIf you are creating a new class, type any new name you like. If you are adding a group of units to a pre-existing class, then start typing the name of the class and it will appear in the dropdown list. The numbers beside each label class in the dropdown list indicate how many instances of that class there are in the dataset. Click the correct class name on the list, then click Set. All selected rows will now have this label.\nNote that after labelling, all selected rows will remain selected. You can un-select all rows quickly with Ctrl+`\nYou can also classify an individual unit by double-clicking the label, subfamily or family cell and typing a string. If the label class already exists you will see it in the dropdown list. Click the list item you want, or use Alt+Down to navigate down the list and Enter to select:\nExemplars Back to table of contents\nEvery new label class you create gets automatically added to the Exemplars view, which functions as a class catalogue to reference during classification. This view has one row per type, with up to 10 randomly-selected exemplars for each row. Go to Classify \u0026amp; manage units \u0026gt; Exemplars to see your exemplars. Click on spectrograms to play the sound, and change playback speed with the speed slider, just as in other views. Click Next 10 random exemplars to randomly select the next batch.\nYou can have Koe open in as many tabs or windows as you want, so it’s easy to have exemplars in one tab and unit table in another, to help you label by comparison. If you make changes in one tab, refresh your other tabs to see the changes.\nFiltering data Back to table of contents\nThe filter box at the top of each program view is a quick and powerful way to filter your data. It takes regular expressions (see https://www.computerhope.com/jargon/r/regex.htm for a crash course on using regex), but it can also be used simply with no technical expertise. Simply mouse over a column header, click the dropdown arrow that appears, and select Filter.\nThen type the string you want to filter by. For example, in the unit table, to filter rows with a certain label, click the Label column dropdown and click Filter, then type a string, e.g.\nlabel: crazysqueak\nYou can also filter by multiple columns at once, separated by semicolons. label: crazysqueak; song: BobtheBat_1\nTo filter date columns (i.e. Date of record and Added columns), use the format from(yyyy-mm-dd)to(yyyy-mm-dd), like this: date: from(2015-01-01)to(2018-12-31)\nTo filter numerical columns, such as ID, use == for \u0026lsquo;exactly\u0026rsquo;, \u0026gt; and \u0026lt; for \u0026lsquo;greater than\u0026rsquo; and \u0026lsquo;less than\u0026rsquo;, respectively.\n^ indicates the start of a string, and $ indicates the end; for example, label: squeak will return rows with ‘squeak’ anywhere in the label, such as ‘crazysqueak’, ‘squeaky’, ‘upsqueak’.\nlabel: ^squeak will return rows beginning with ‘squeak’, such as ‘squeaky’.\nlabel: squeak$ will return rows ending with ‘squeak’, such as ‘crazysqueak’, ‘upsqueak’.\nlabel: ^squeak$ will return rows with exactly ‘squeak’ (nothing before or after).\nlabel: ^$ is a handy way of finding rows with blank labels.\nView all songs Back to table of contents\nClick on View all songs. Here you can see all your songs, with one row per song. You will see the song sequence structure as a sequence of unit labels.\nTo illustrate, the figure below shows a song (upper panel) and how it is represented in View all songs as a string of labels (lower panel).\nClick on a unit to play it and see its spectrogram:\nClick the play button at the start of a sequence to play the entire song. Use the filter to search for songs of a particular unit sequence, e.g. sequence:\u0026quot;trill\u0026quot;-\u0026quot;dragon\u0026quot; will filter for all songs with a trill unit immediately followed by a dragon unit. This is useful for finding songs with a shared pattern and exploring syntax.\nMine sequence structure Back to table of contents\nAs one way of exploring rules that may govern song structure (i.e. syntax), Koe uses the cSPADE (constrained Sequential Pattern Discovery using Equivalence classes) algorithm (Zaki, 2001) to discover commonly-occurring sequences in a set of songs.\nWe consider a sequence to be an ordered list of acoustic units denoted as A⇒B⇒C⇒...⇒x.\nA sequence rule has two parts: a left side (what comes before) and a right side (what follows). The rule states that when the left side occurs, the right side follows\n[Left side]⇒[Right side]\nFor example, take A⇒B ⇒ C. This rule states that when the sequence A⇒B occurs, C comes next.\nThe left side can be a sequence of any length, but in our current implementation the right side is always one unit long For example:\nA⇒B (when A occurs, B comes next)\nor\nA⇒B⇒C⇒D⇒E⇒F⇒G⇒H⇒I⇒J (when A⇒B⇒C⇒D⇒E⇒F⇒G⇒H⇒I occurs, J comes next).\nWe plan to relax this restriction in a future update to allow chains of units on the right side.\nThe cSPADE algorithm calculates the credibility of sequence rules. Credible rules have a large confidence factor, a large level of support and a value of lift greater than one (as defined below). Let\u0026rsquo;s take the example rule A⇒B ⇒ C\nSupport: The level of support is the proportion of songs in the database that contain the entire sequence A⇒B⇒C at least once.\nConfidence: The strength of an association is defined by its confidence factor (hereafter ‘confidence’), which is the proportion of those songs containing A⇒B that also contain A⇒B⇒C.\nLift: Lift is a measure of the strength of the association relative to chance. It is equal to the confidence of the rule, divided by the proportion of songs containing the right side. Thus it gives the ratio of (i) the proportion of songs the transition from A⇒B⇒C occurs in, versus (ii) the proportion of songs expected to contain A⇒B⇒C by chance association.\nThink of our example rule, A⇒B⇒C. Imagine a dataset with 100 songs, where 10 of those songs contain A⇒B, 7 contain A⇒B⇒C, and 20 songs contain C.\nThe support is 0.07 (i.e. A⇒B⇒C occurs in 7/100 songs). The confidence is 0.7 (i.e . A⇒B⇒C occurs in 7/10 songs containing A⇒B). The lift is the confidence (0.7) divided by the proportion of songs containing C (0.2), which is 3.5. In other words, the association A⇒B⇒C occurs in 3.5 times as many songs as expected by chance association of A⇒B and C.\nTo demonstrate cSPADE with real data, consider the ten songs shown in the figure below to be a population of songs.\nThe rule\nStutter⇒Waah(magpie) ⇒ Pipe(B6)\nhas a support of 0.4 since the entire sequence occurs in four of the 10 songs. The rule has a confidence of 0.8, because in four of the five songs that contain Stutter⇒Waah(magpie), the transition to Pipe(B6) occurs. The proportion of songs with Pipe(B6) is 0.6, so the lift of this rule is 0.8/0.6=1.33. That is, the association Stutter⇒Waah(magpie) ⇒ Pipe(B6) occurs in 1.33 times as many songs as expected by chance association of Stutter⇒Waah(magpie) and Pipe(B6).\nVisualise sequence structure as a network Back to table of contents\nTwo-unit associations from cSPADE can be visualised using a directed network. The network models the direction and strength of association between pairs of units, across a population of songs. Units are represented by nodes (vertices) which are joined by lines (edges) if the units occur consecutively (by default Koe shows only those sequences that occur in at least 1% of songs). The order of units is indicated by arrow direction, and strength of association between units (lift) is represented by edge thickness. Visually cluttered networks can be simplified using the filter, e.g. to show only associations with high lift.\nControl network appearance with the following controls:\n   Network control Description     Separate units by actual time (on/off) Activate Max/Min gap controls   Max gap (10ms-1000ms) The maximum gap length between units before they are no longer considered to be in the same sequence   Min gap (1ms-100ms) The minimum gap length between units below which they are no longer considered to be in the same sequence   Show pseudo begin and end (on/off) Displays start and end nodes, to visualise which classes tend to start and/or end songs   Show node labels (on/off) Toggle visibility of node labels   Reactive pseudo start (on/off) Pulls starting units towards the start node   Reactive pseudo end (on/off) Pulls ending units towards the end node   Centering (on/off) Prevents the network clumping in a corner of the screen   Repulsion (1-50) Determines the extent to which nodes actively repel each other   Distance (10%-400%) Determines how ‘spread out’ the nodes are    Nodes can also be dragged around with the mouse to optimise network appearance.\nExporting your data Back to table of contents\nYou can export the data from each program view as a CSV file by clicking Export entire table (csv) or Export filtered subset (csv) on the active controls pane. The format of exported tables matches how the data appears in Koe; for example, data exported from the Unit table view will match the column and row order of the unit table as it appears in Koe.\nCopy songs to another database Back to table of contents\nFirst make sure you have created the database which you want to copy songs to. Then, under View all songs, select the songs you want to copy (1), and click Copy to another database (2). Type the name of the database to copy to (3) then click Yes (4)\nMake database subsets (collections) Back to table of contents\nYou can make collections of units as convenient subsets for analysis. Under Classify \u0026amp; manage units \u0026gt; Unit table, select all the units you want to be in the collection, then click Make a Collection from selected units on the active controls pane. Type a name for the collection, then click Set. Note the Collection name must be unique; if the collection name has already been created (by any users, not just you), the name will not \u0026lsquo;take\u0026rsquo;. Also note that a Collection is not separate from the parent database – any changes you make to the Collection will be reflected in the parent database. Therefore you should take care when naming the collection so that you will remember which parent it belongs to. You can rename and delete collections under Manage your database.\nIf you want to create a subset that you can modify independently from the parent database, go to View all songs and copy songs to another database. Then you can edit freely without affecting the original database. To achieve this, do the following:\n  Create a new database under Manage your database.\n  Go to View all songs.\n  Under the Current database dropdown list on the controls pane, choose the database you want to copy songs from.\n  Select the songs you want to copy by checking their checkboxes.\n  Click Copy to another database on the active controls pane.\n  Type the name of the newly created database you want to copy songs to.\n  Click copy\n  Keyboard shortcuts Browser shortcuts In Chrome: |Hotkey|Action| |\u0026ndash;|\u0026ndash;| |Ctrl+Page Up / Ctrl+Page Down|Toggle between browser tabs| |Alt+D, then Alt+Enter|Duplicate current tab| |F5| Refresh current tab |F12 (brings up developer panel) + long-click browser refresh button \u0026gt; Empty Cache and Hard Reload|Can resolve Koe glitches. Try it—if you don\u0026rsquo;t mind emptying your cache\nNavigating data tables generally These hotkeys are common to all views with data tables. Additional view-specific hotkeys are given in the following sections. |Hotkey|Action| |\u0026ndash;|\u0026ndash;| |← ↑ ↓ →|Navigate the table grid| |Home|Jump to first cell in row |End|Jump to end cell in row| |Page Up/Page Down|Page the table one screen-height at a time| |click on table header, Shift+click on subsequent headers|Sort by multiple columns, in the priority order headers were clicked|\nView all songs view    Hotkey Action     Spacebar Check/uncheck checkbox when checkbox cell is active   Shift+click on checkbox Select a range (i.e. selects all rows between the last selected checkbox and the Shift+clicked checkbox)    Segmenting songs into units—Segment songs into units view    Hotkey Action     Shift while mousing over segmented unit box Reveal unit selection box start/end handles; click and drag handles while holding Shift to adjust   Hotkey coming/Hotkey coming Page spectrogram one screen-width forwards/backwards, to navigate through a recording quickly while segmenting units   Spacebar Check/uncheck checkbox when checkbox cell is active   Shift+click on checkbox Select a range (i.e. selects all rows between the last selected checkbox and the Shift+clicked checkbox)    Classifying units—Unit table view Units can have three sets of labels at once, using the three label columns: Label, Subfamily and Family. You can use these to label at different scales, e.g. fine-scale, intermediate-scale and broad-scale classification. Or you may choose to use these columns for other purposes—it\u0026rsquo;s up to you!\n   Hotkey Action     Spacebar Play audio (when spectrogram cell is active); Check/uncheck row selection checkbox (when checkbox cell is active)   Shift+click on checkbox Select a range (i.e. selects all rows between the last selected checkbox and the Shift+clicked checkbox)   Ctrl+Shift+L bulk-label all selected rows in Label column (e.g. fine-scale classification)   Ctrl+Shift+S bulk-label all selected rows in Subfamily column (e.g. intermediate-scale classification)   Ctrl+Shift+F bulk-label all selected rows in Family column (e.g. broad-scale classification)   Ctrl+ Grave accent key (same key as ~ but without shift) Deselect all currently selected rows    Suggestions for other hotkeys? Let us know!\n=================================================\nJUMP TO: [[Analysis Tutorial: Case Study of NZ bellbird song]].\nJUMP TO: [[Frequently Asked Questions]]\nJUMP TO: [[What\u0026rsquo;s new in Koe?]]\n \r"
},
{
	"uri": "http://koe-wiki.github.io/features/",
	"title": "Acoustic features",
	"tags": [],
	"description": "",
	"content": "Table of content \rFrequency domain features\r\r\r\r\rTime domain features\r\r\r\r\rPerceptual based features\r\r\r\r\r"
},
{
	"uri": "http://koe-wiki.github.io/features/frequency-domain/",
	"title": "Time domain features",
	"tags": [],
	"description": "",
	"content": "The time domain represents changes in the signal over time, or more precisely, the changes in air pressure when the sound reaches a microphone, which is measured by voltage value. Features in this category are necessarily extracted directly from the oscillogram without any transformation. Thus, they tend to have excellent time resolution and low computational cost.\nZero Crossing Rate (ZCR) Defined as the number of zero crossings within one second is an efficient way to approximate the dominant frequency\nTemporal Envelope Is the smoothed amplitude calculated by a moving window over the absolute value of the signal\u0026rsquo;s waveform. For signals that have relatively low and constant background noise, the temporal envelope provides a simple way to detect intervals of silence and thus could be used for audio segmentation.\nShort-time energy Is the mean square of the amplitude of the waveform over one frame\n$$p\\triangleq\\frac{\\sum_{i=t}^{t+d-1} {s_i}^2}{d}$$\nLoudness Is the root mean square of the amplitude of the waveform over one frame\n$$p\\triangleq\\sqrt{\\frac{\\sum_{i=t}^{t+d-1} {s_i}^2}{d}}$$\nTemporal Centroid Is the point in time where most of the signal energy is concentrated, calculated as the time average over the envelope of a signal in seconds\nLog Attack Time is the logarithm of time duration from the beginning of the sound to the point where the envelope reaches its first maximum. LAT characterises the attack of sound, which can be smooth or sudden.\n"
},
{
	"uri": "http://koe-wiki.github.io/analysis-tutorial/",
	"title": "Analysis Tutorial: Case Study of NZ bellbird song",
	"tags": [],
	"description": "",
	"content": "NZ bellbird (Anthornis melanura) male (front) and female (back).\nHere we give a detailed walk-through tutorial of the analyses performed in Fukuzawa et al. (2020) https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.13336.\nTable of Contents  Getting started Trying out the filter   What is the overall unit diversity for male and female bellbirds across all seven sites?  Find the total number of classes Make a database subset for males Make a database subset for females Find how many classes are sung by males Find how many classes are sung by females Calculating repertoire overlap   How many male unit types are now shared between Tawharanui and source population Hauturu?  Make a subset for males on LBI Make a subset for males on TAW Find repertoire size for males on LBI Find repertoire size for males on TAW Calculate LBI-TAW repertoire overlap   Evaluating song structure in bellbirds  Make a subset collection for males on TMI: Make a subset collection for females on TMI: Produce cSPADE table and network for males on TMI: Produce cSPADE table and network for females on TMI: Compare male and female networks Explore male cSPADE table Explore female cSPADE table   Tutorial conclusions  Table of contents generated with markdown-toc\nGetting started To follow along in Koe with the same dataset:\n  Click here to download a zip file of database labels.\n  Go to Koe (https://koe.io.ac.nz).\n  Sign in (if you aren\u0026rsquo;t already).\n  Navigate to Manage your database.\n  Enter invitation code “KOE Case Study” (without quotes)\n  Select the Bellbirds_Case_Study database from the Database panel by clicking the round checkbox next to its name.\n  Click Load ZIP beneath the Saved versions panel.\n  Locate the zip file you downloaded in step 1.\n  Click Open.\n  You have now loaded the label data for the units in the database and will be able to see unit labels.\nPlease note: by accessing the datasets in this tutorial you agree not to use the data in any form of publication, except by express consent of the authors Wesley Webb and Yukio Fukuzawa.\nTrying out the filter   Navigate to Unit table view (Classify \u0026amp; manage units \u0026gt; Unit table)\n  Make sure that the current database is Bellbirds_Case_Study; select it from the Current database dropdown on the navigation pane if not.\n  Data can be explored quickly using Koe\u0026rsquo;s filter, which can combine different criteria specified by a regular expression. For example, duration:\u0026lt;100; sex:F; label:Downsqueak returns only units with duration shorter than 100ms, vocalised by female birds, and label containing Downsqueak. Try it:\nBelow, we use the filter to compare male and female unit diversity across the whole metapopulation, then between males at two sites.\nWhat is the overall unit diversity for male and female bellbirds across all seven sites? Back to table of contents\nFind the total number of classes Exemplars view (Classify \u0026amp; manage units \u0026gt; Exemplars) displays the catalogue of unit classes, with a count of classes top left (where the arrow is pointing). This reveals 754 fine-scale classes in total:\nNow we want to see how many of the 754 are sung by males, how many by females, and how many are shared between the sexes.\nMake a database subset for males Make subset \u0026lsquo;collections\u0026rsquo; by sex as follows:\n Go to Classify \u0026amp; manage units \u0026gt; Unit table   Type sex:M in the filter box to filter for all units sung by males\n  Select the checkbox header to select all filtered rows.\n  Click Make a Collection from selected units on the active controls pane.  Type a unique name for the collection (e.g. \u0026ldquo;yourusername_Bellbirds_Males\u0026rdquo;). Note the Collection name must be unique; if the collection name has already been created (by any users, not just you), the name will not \u0026lsquo;take\u0026rsquo;.   Click Set.\n  Deselect the selected rows by clicking the checkbox header or typing ctrl+`.\n  Make a database subset for females Make a subset for units sung by females in exactly the same way:\n  Replace the filter withsex:F to filter for all units sung by females.\n  Select the checkbox header to select all filtered rows.\n  Click Make a Collection from selected units\n  Type a unique name for the collection (e.g. \u0026ldquo;yourusername_Bellbirds_Females\u0026rdquo;)\n  Click Set.\n  Deselect the selected rows by clicking the checkbox header or typing ctrl+`.\n  Find how many classes are sung by males Go back to Exemplars view (Classify \u0026amp; manage units_\u0026gt;_Exemplars).\n  Click on the Current database dropdown in the active controls pane.\n  Select the male subset Collection you made.\n  The class count at the top left (where the arrow is pointing) shows there are 517 fine-scale classes for males:\nFind how many classes are sung by females Repeat the process for the female subset. The class count shows there are 415 fine-scale classes for females:\nCalculating repertoire overlap If there are 754 fine-scale classes overall, with 517 sung by males and 415 sung by females, there must be 178 (24%) sung by both (517 + 415 – 754 = 178).\nHere\u0026rsquo;s another slightly more involved question:\nHow many male unit types are now shared between Tawharanui and source population Hauturu? Back to table of contents\nIn 2005, bellbirds from Hauturu (also known as Little Barrier Island; LBI) naturally recolonized Tawharanui Peninsula (TAW) following pest control. At that point, repertoires of source and founder populations were indistinguishable (Brunton et al., 2008). After a decade of potential cultural divergence, it is of interest to assess the current degree of repertoire overlap between the sites.\nWe will do this for males.\nNote: All our songs have filenames that start with a three-character site name (LBI and TAW in this case) followed by an _. We use this feature to filter for songs from different sites.\nMake a subset for males on LBI First we will go to Unit table view (Classify \u0026amp; manage units \u0026gt; Unit table) and make a \u0026lsquo;collection\u0026rsquo; containing units sung by males from LBI:\n  Type sex:M; song:LBI_ in the filter.\n  In the resulting unit table, select all rows by clicking the checkbox header.\n  Click Make a Collection from selected units on the controls pane.\n   Name the collection something logical and unique (e.g. \u0026ldquo;yourusername_LBI_M\u0026rdquo;) and click Set. Note the Collection name must be unique; if the collection name has already been created (by any users, not just you), the name will not \u0026lsquo;take\u0026rsquo;.   Deselect all rows by unchecking the checkbox header, or pressing Ctrl+`\n  Make a subset for males on TAW   Replace the filter with sex:M; song:TAW_\n  In the resulting unit table, select all rows by clicking the checkbox header\n  Click Make a Collection from selected units on the controls pane\n   Name the collection something logical and unique (e.g. \u0026ldquo;yourusername_TAW_M\u0026rdquo;) and click Set   Deselect all rows by unchecking the checkbox header, or pressing Ctrl+`.\n  Find repertoire size for males on LBI To find the repertoire size of the male LBI collection, go to Exemplars view (Classify \u0026amp; manage units \u0026gt; Exemplars):\n  Click on the current database dropdown.\n  Select your collection for LBI males, whatever you named it.\n  In the top left (where the arrow is pointing) is the count of class types in the collection. In this case, 141 male fine-scale unit classes on LBI:\n  Export the Exemplars view data as a .csv file. Click Export entire table (csv) on the active controls pane.  Find repertoire size for males on TAW Open the Tawharanui (TAW) male collection in Exemplars view (Classify \u0026amp; manage units \u0026gt; Exemplars) exactly as previous:\n  Click on the Current database dropdown\n  Select your collection for TAW males, whatever you named it\n  In this case we find 104 male fine-scale unit classes on TAW:\n  Export this Exemplars view data, too. Click Export entire table (csv) on the active controls pane.  Calculate LBI-TAW repertoire overlap The two downloaded .csv files will be in the downloads bar at the bottom of your browser. (You can also find them in your Downloads folder on your computer).\nThe files contain two columns: (1) the list of classes present in the collection, and (2) the counts of those classes.\nThere are several ways to find the classes shared between the two lists. You could do it in Excel with formulas. Here we will read the two lists into R and use intersect to find the classes common to both lists.\n Read in the LBI and TAW csv files (Note: replace filepaths with your own), and store them as objects LBI_Males and TAW_Males:  \rLBI_Males \u0026lt;- read.csv(\u0026quot;C:/Downloads/koe-exemplars-2019-03-07_15-26-57.csv\u0026quot;,header=TRUE)\rTAW_Males \u0026lt;- read.csv(\u0026quot;C:/Downloads/koe-exemplars-2019-03-07_15-27-07.csv\u0026quot;,header=TRUE)\rNow we find the the classes that are common to both files. We are only interested in the first column (the list of classes) from each file; hence the [,1] in the code which extracts the first column:  \rShared_classes \u0026lt;- intersect(LBI_Males[,1], TAW_Males[,1])\rThe list of shared classes is now stored in the object Shared_classes, which looks like this:  \u0026gt; Shared_classes\r[1] \u0026quot;Chump\u0026quot; \u0026quot;Click\u0026quot; [3] \u0026quot;Cough(clickhigh)\u0026quot; \u0026quot;Cough(short)\u0026quot; [5] \u0026quot;Down(slightcurve+click)\u0026quot; \u0026quot;Downsqueak(chumpbit)_Pipe(high)\u0026quot;\r[7] \u0026quot;Ghh_Chump\u0026quot; \u0026quot;Pipe(A#[6])\u0026quot; [9] \u0026quot;Pipe(A#[6]+)\u0026quot; \u0026quot;Pipe(A#[6]bent)\u0026quot; [11] \u0026quot;Pipe(A[6])\u0026quot; \u0026quot;Pipe(A[6]+)\u0026quot; [13] \u0026quot;Pipe(B[6])\u0026quot; \u0026quot;Pipe(B[6]+)\u0026quot; [15] \u0026quot;Pipe(C[7])\u0026quot; \u0026quot;Pipe(D#[6])\u0026quot; [17] \u0026quot;Pipe(D#[6]+)\u0026quot; \u0026quot;Pipe(D[6])\u0026quot; [19] \u0026quot;Pipe(D[6]+)\u0026quot; \u0026quot;Pipe(D[6]bent)\u0026quot; [21] \u0026quot;Pipe(D[7])\u0026quot; \u0026quot;Pipe(E[6])\u0026quot; [23] \u0026quot;Pipe(F#[6])\u0026quot; \u0026quot;Pipe(F#[6]+)\u0026quot; [25] \u0026quot;Pipe(F#[7]+)\u0026quot; \u0026quot;Pipe(G#[6]+)\u0026quot; [27] \u0026quot;Pipe(G[6])\u0026quot; \u0026quot;Pipe(G[6]+)\u0026quot; [29] \u0026quot;Pipe_Upslope_Pipe\u0026quot; \u0026quot;Stutter\u0026quot; [31] \u0026quot;Stutter_Stutter(LBITAW+nownow)\u0026quot; \u0026quot;Stutter_Stutter_Stutter\u0026quot; [33] \u0026quot;Tink\u0026quot; \u0026quot;UNKNOWN\u0026quot; [35] \u0026quot;Upsqueak(cleanglass)\u0026quot; There are 35 Tawharanui male unit classes shared with Hauturu (34% of the male Tawharanui population repertoire; 35/104 = 34%).\nIn other words, since colonization 66% of the unit classes sung by males at Tawharanui appear to have been innovated.\nEvaluating song structure in bellbirds Back to table of contents\nTo help you analyse sequence structure in detail, Mine sequence structure view shows the strength of association between unit classes using the cSPADE algorithm (Zaki, 2001), and produces network visualisations.\nHere we will use Mine sequence structure to compare the sequence structure of male and female bellbird song on Tiritiri Matangi Island (TMI).\nFirst, exactly as in previous sections, we need to make collections (database subsets) for males and females on TMI.\nMake a subset collection for males on TMI:   Go to Unit table view (Classify \u0026amp; manage units \u0026gt; Unit table)\n  Type sex:M; song:TMI_ in the filter\n  In the resulting unit table, select all rows by clicking the checkbox header\n  Click Make a Collection from selected units on the controls pane\n  Name the collection something logical and unique and click Set\n  Deselect all rows by unchecking the checkbox header, or pressing Ctrl+`\n  Make a subset collection for females on TMI:   Replace the filter with sex:F; song:TMI_\n  In the resulting unit table, select all rows by clicking the checkbox header\n  Click Make a Collection from selected units on the controls pane\n  Name the collection something logical and unique and click Set\n  Deselect all rows by unchecking the checkbox header, or pressing Ctrl+`\n  Produce cSPADE table and network for males on TMI:  Click on Mine sequence structure in the navigation pane.   Click on the Current database dropdown.\n  Select your collection for TMI males, whatever you named it.\n  You will see a table of class associations on the left, and a network visualisation on the right.\nThe cSPADE output table enumerates the strength of association for sequences of units. For details of the cSPADE algorithm see this previous section.\nThe interactive network visualisation shows all the 2-unit associations from the cSPADE table. The network models the direction and strength of association between pairs of units, across a population of songs (in this case, the population consists of males on TMI).\nUnits are represented by nodes (the circles) which are joined by lines (edges) if the units occur consecutively within songs. The order of units is indicated by arrow direction, and strength of association between units (lift) is represented by edge thickness.\nAgain, see this section to be reminded of definitions and this section on adjusting network appearance.\nBy default the network shows only those sequences that occur in at least 1% of songs. In this case, there are a high number of associations, leading to a cluttered network. We will increase the threshold to visualise only those associations that occur in more than 5% of songs.\nType support:\u0026gt;0.05 in the filter box and observe how the network is now much simpler:  Export the filtered subset of the cSPADE table by clicking Export filtered subset (csv) in the active controls pane:  Produce cSPADE table and network for females on TMI: We want to compare networks for males and females side-by-side, so let\u0026rsquo;s duplicate the current Koe browser tab to preserve the male network before we create the female network.\n Right-click the tab then click Duplicate from the dropdown menu (the menu will look different in different browsers).   If you are using two computer monitors, drag the new tab to the second monitor, so that you can see both simultaneously. One tab will be for the male network (already produced) and the other for the female network.\n  On the tab you want to produce the female network, click on the Current database dropdown.\n  Select your collection for TMI females, whatever you named it.\n  Similarly to the male network, at the default threshold of 1% occurrence (present in at least 1% of songs), the network is cluttered with many associations: To be comparable with the male network, we will increase the threshold to visualise only those associations that occur in more than 5% of songs.\nType support:\u0026gt;0.05 in the filter box  Export the filtered subset of the cSPADE table by clicking Export filtered subset (csv) in the active controls pane:  Compare male and female networks At this threshold, the songs of both sexes contain some nodes with many connections; these unit classes (e.g. Dragon in males, Stutter in females) can be preceded/followed by many different classes.\nBoth sexes also contain nodes which are strictly preceded/followed by a certain class; for example, in males Waah(mid+A) is always followed by Dragon, and in females Peakwhistle(TMI) is always preceded by Stutter.\nWe can see that both sexes sing repeated units, represented by looped lines:\nFor males: Dragon, Stutter, Cough(TMI+click) For females: Stutter, Stutter(wavey), Pipe(D#[7]+), Pipe(E[7])\nA notable difference in the female network is this chain of strongly connected nodes, upper left-hand-side of the graph:\nKillem → Pipe(C#[7]+) → Upsqueak_Flatsqueak(1) → Downsqueak(twopart) → Down_Click_Stepdown → Stepdown(C[7]-C[6]).\nThe thick lines between these pairs of classes on the graph indicate high values of lift; that is, each two-unit association occurs much more often than would be expected by chance. Note this does not necessarily imply that the entire chain occurs in any one song, but rather that each consecutive pair is strongly associated.\nExplore male cSPADE table   Find the male cSPADE table csv file you previously exported. It will be in your computer\u0026rsquo;s Downloads folder.\n  Open it in Excel\n  Sort the table by the chainlength column from lowest to highest, then secondarily by the support column from highest to lowest:\n  The sorted data should look like this:\nThese associations at the top of the table (with chain length of 1) are not true associations per se, as they are only one unit long. They give useful information about how the number and proportion of songs those unit classes occur in (transcount and support columns, respectively). However, confidence and lift values cannot be calculated because these require associations with two or more units (hence the nonsensical -1 values in these columns).\n__PSEUDO_START__ and __PSEUDO_END__ are not true classes, but mark the start and end points of the song. As every song has a start and end, both have a support of 1 (meaning they occur in 100% of songs).\nThe third row of the table shows that Dragon occurs in 201 songs (transcount column), which is 51.7% of all the songs in the male TMI population (support column). By this measure it is the most prevalent male syllable class on TMI; next most prevalent are Cough(TMI+click) (40.1% of songs), Downsqueak(hook) (34.7% of songs), and so on down the list.\nScroll down or filter the table to find associations with a chain length of 2. These are the associations visualised in the network diagram.  The sequence:\nCough(TMI+click) ⇒ Downsqueak(hook)\nhas a support of 0.32, which means this sequence occurs in 32% of songs.\nIt has a confidence of 0.81, which means that when Cough(TMI+click) occurs, Downsqueak(hook) comes next in 81% of songs.\nIt has a lift of 2.33, which means that Downsqueak(hook) follows Cough(TMI+click) in 2.33 times the number of songs as expected by chance.\nNote: In the notation of Zaki (2001) this association would be written [(Cough(TMI+click))]⇒[(Cough(TMI+click))(Downsqueak(hook))]\nMale song tends to start with one of four syllable types\nBy looking at two-unit associations beginning with __PSEUDO_START__, we can see which syllables tend to occur at the start of male songs:\nThe \u0026lsquo;sequence\u0026rsquo;\n__PSEUDO_START__ ⇒ Stutter\nhas a support of 0.29, which means that in 29% of songs in this male population, __PSEUDO_START__ ⇒ Stutter occurs (that is, Stutter is the starting syllable).\nIt also has a confidence of 0.29, which means given __PSEUDO_START__, Stutter follows in 29% of songs (in this case, this means the same as support).\nIt has a lift of 0.95, which means that Stutter follows __PSEUDO_START__ in 95% as many songs as expected by chance, based on the frequency of Stutter in the dataset.\nThe other three starting syllables are:\n__PSEUDO_START__ ⇒ Cough(TMI+click) (support=0.23, confidence=0.23, lift=0.57)\n__PSEUDO_START__ ⇒ Downsqueak(bitunder)_Pipe(low) (support=0.10, confidence=0.10, lift=0.45)\n__PSEUDO_START__ ⇒  Waah(rough) (support=0.08, confidence=0.08, lift=0.30)\nThree syllable types tend to end male song\nBy looking at two-unit associations ending with __PSEUDO_END__, we can see which syllables tend to occur at the end of male songs:\nMoustache ⇒ __PSEUDO_END__ (support=0.12, confidence=0.81, lift=0.81)\nDragon ⇒ __PSEUDO_END__ (support=0.30, confidence=0.57, lift=0.57)\nCough(TMI+click) ⇒ __PSEUDO_END__ (support=0.12, confidence=0.29, lift=0.29)\nScroll down the table to find associations with a chain length of 3.  The sequence:\nTrill ⇒ Dragon ⇒ Moustache\nhas a support of 0.08, which means this sequence occurs in 8% of songs.\nIt has a confidence of 0.86, which means that when Trill ⇒ Dragon occurs, Moustache comes next in 86% of songs.\nIt has a lift of 5.9, which means that Moustache follows Trill ⇒ Dragon in 5.9 times the number of songs as expected by chance.\nNote: In the notation of Zaki (2001) this association would be written [(Trill)(Dragon)]⇒[(Trill)(Dragon)(Moustache)]\nExplore female cSPADE table   Find the female cSPADE table csv file you previously exported. It will be in your computer\u0026rsquo;s Downloads folder.\n  Open it in Excel\n  Sort the table by the chainlength column from lowest to highest, then secondarily by the support column from highest to lowest:\n  The sorted data should look like this:\nThe third row of the table shows that Stutter occurs in 129 songs (transcount column), which is 93.4% of all the songs in the female TMI population (support column). By this measure it is the most prevalent female syllable class on TMI (compared to 51.7% for the most prevalent class in males).\nThe next most prevalent female classes are Chiup (59.4% of songs), Downcurve(doublevoiced) (36.2% of songs), and so on down the list.\nScroll down to find associations with a chain length of 2. These are the associations visualised in the network diagram.  Here we can see the importance of Stutter syllable types for the structure of female bellbird song.\nStutter almost always starts female songs\n__PSEUDO_START__ ⇒ Stutter\nhas a support of 0.91, which means that in 91% of songs in this population, Stutter is the starting syllable.\nIt also has a confidence of 0.91, which means given the start of a song (__PSEUDO_START__), Stutter follows in 91% of cases.\nIt has a lift of 0.97, which means that Stutter follows __PSEUDO_START__ 97% as often as expected by chance.\nStutter tends to repeat in female songs\nStutter ⇒ Stutter is common (support=0.85, confidence=0.91, lift=0.97).\nIn fact, if we scroll down the table we see that Stutter is commonly repeated many times:\nFor example,\nStutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter\noccurs in 29% of songs\nStutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter ⇒ Stutter\noccurs in 15% of songs.\nStutter sometimes ends female songs\nStutter ⇒ __PSUEDO_END__ occurs in 38% of songs (support=0.38, confidence=0.40, lift=0.40).\nTutorial conclusions Back to table of contents\nThese examples have illustrated a some simple ways to assess unit repertoire size and repertoire sharing, and demonstrated how to examine sequence structure using the cSPADE algorithm and network visualisations. Data exported from Koe enable many more kinds of analyses in other software.\nFor example:\n  EstimateS (http://viceroy.eeb.uconn.edu/estimates/) can be used to estimate total unit richness in a population by robustly extrapolating from the Koe classification data.\n  From the unit features extracted by Koe, distributions of unit features could be plotted in R or Python and compared between groups\n  In Excel unusual syntax could be identified in the cSPADE table as one means of identifying potential classification errors\n   \r"
},
{
	"uri": "http://koe-wiki.github.io/features/perceptual-features/",
	"title": "Perceptual based features",
	"tags": [],
	"description": "",
	"content": "Linear Predictive Cepstral Coefficients Is the Fourier transform of the log magnitude of the autoregressive power spectrum of the excitation source in a linear prediction model.\n"
},
{
	"uri": "http://koe-wiki.github.io/faqs/",
	"title": "Frequently Asked Questions",
	"tags": [],
	"description": "",
	"content": "How do I report bugs or suggest features? Your bug reporting and feature requests help us make Koe a joy to use!\nLive chat with us at our Facebook page www.facebook.com/KoeSoftware. Alternatively, send us an email at koe@io.ac.nz. Check out the current list of known bugs and feature requests under the Issues tab in GitHub.\nCan I install Koe on my local machine? Yes you can, but it will take a wee bit of coding know-how to set it up. See: https://github.com/fzyukio/koe#how-to-install-run-and-deploy-this-on-your-own\nI\u0026rsquo;m missing certain buttons on the navigation pane\u0026hellip; If you don\u0026rsquo;t see certain buttons, e.g. \u0026lsquo;Upload \u0026amp; split raw recordings\u0026rsquo; or \u0026lsquo;Upload songs\u0026rsquo; (left hand side of the screen), it\u0026rsquo;s probably because the database is owned by another user and they haven\u0026rsquo;t granted you full permissions.\nFor instructions on how to grant permissions, see: https://github.com/fzyukio/koe/wiki#add-collaborators-to-a-database-and-set-permission-levels\nHow can I adjust spectrogram settings to suit the Hz range of my species? Currently you can adjust spectrogram colourmap, contrast, and time-axis zoom. We recognise the need to be able to specify spectrogram floor/ceiling, especially for species with low-pitched vocalisations. We are working on expanding the range of spectrogram settings. Contact us to tell us more about the requirements of your species, so that we can make sure Koe is useful for you.\nI have already segmented units in other software. Can I import this information into Koe? Rejoice! You can import segmentation data as unit start/end-points in a csv file, thereby avoiding having to re-segment units in Koe.\nSee https://github.com/fzyukio/koe/wiki#import-unit-segmentation-from-csv\nHow do I get my tSNE ordination to cluster nicely? How clustered your data appears in the ordination (Classify \u0026amp; manage units \u0026gt; Ordination) will depend on a few things:\nThe set of unit features you extract This is what is used to calculate unit similarity. Some combinations of features may be better at separating unit categories than others.\nThe value of \u0026lsquo;perplexity\u0026rsquo; you set for the tSNE tSNE has a global variable called \u0026lsquo;perplexity\u0026rsquo; which influences the appearance of the ordination. This variable is used universally for all data points. A small value can cause bona fide clusters to break up, a big value can lead to several clusters being put together.\nHow many data points you have Sometimes, having many thousands of data points can result in a big cloud-like blob rather than neat clusters.\n Here are some things to try:\n\u0026ndash; Change which unit features are extracted. If in doubt, extract all features, using all aggregation methods (https://github.com/fzyukio/koe/wiki#extract-unit-features).\n\u0026ndash; Try changing the value of the \u0026lsquo;perplexity\u0026rsquo; variable (https://github.com/fzyukio/koe/wiki#construct-ordination)\n\u0026ndash; If you are experiencing some good clusters, but other regions of the ordination are a \u0026lsquo;big blob\u0026rsquo;, try this:\n Make a selection of the big blob in the ordination Click the View table button, which will open a Unit table containing only the selected units. Click on the top checkbox button to select all those units Click Make collection from selected units on the left hand panel.  Now you can use this collection as a database and run feature extraction with different features—and ordination with different values of perplexity—to see if the cloud of points can break up into clusters.\n"
},
{
	"uri": "http://koe-wiki.github.io/whats-new/",
	"title": "What&#39;s new in Koe?",
	"tags": [],
	"description": "",
	"content": "We update this page regularly to report new features and bug fixes. 06/04/2020 Added keyboard shortcuts section to wiki (https://github.com/fzyukio/koe/wiki#keyboard-shortcuts)\n01/04/2020 — New Release — Version 5.4.1 New stuff –Report bugs or suggest features by chatting with us directly at facebook.com/KoeSoftware (just hit the Send message button). You can still email us at koe@io.ac.nz if you prefer.\n–A new Frequently Asked Questions page (https://github.com/fzyukio/koe/wiki/Frequently-Asked-Questions). We will update this regularly.\n–Import unit segmentation start/end points. If you have already segmented units using other software, you can import the unit segmentation start/end point data into Koe, and avoid doing it over again in Koe. This is now properly documented on the wiki: https://github.com/fzyukio/koe/wiki#import-unit-segmentation-from-csv\n–Automatically split long wav files into smaller chunks. Some of us work with very long, cumbersome recordings. Yukio has made a handy tool which allows you to automatically split long wav files into sections of specified length, with a specified amount of overlap between sections. This is a standalone tool, not inside of Koe (because it runs on your computer, not in your browser). To run the tool you will need to install Python. After that it\u0026rsquo;s just a simple line of code that you type into your command prompt. The tool and instructions are here: https://github.com/fzyukio/split-songs\n–Database hierarchy makes more sense. If you change \u0026lsquo;sex\u0026rsquo; or \u0026lsquo;species\u0026rsquo; column info associated with an individual animal, the changes will be reflected for all rows in the table belonging to that individual (you may need to refresh your browser to see the change propagate). This prevents mistakes, such as the same individual being ascribed male sex in some rows and female sex in other rows. It also means you only have to enter sex and species information for an individual in one row, saving time.\n–New \u0026lsquo;Added\u0026rsquo; data column—to record the date a recording was added to the database. Handy for finding recent files versus files added a long time ago.\n–Filter data by date. You can now filter your data in Classify \u0026amp; manage units \u0026gt; Unit table by date in \u0026lsquo;Date of record\u0026rsquo; and \u0026lsquo;Added\u0026rsquo; columns. Use this format, e.g. date: from(2015-01-01)to(2018-12-31)\n–Users can now delete databases\n–Species name column is no longer constrained to two-part \u0026ldquo;Genus species\u0026rdquo; naming scheme; name species any way you want\nFixed stuff –Audio upload issues are fixed. This includes issues with uploading large files. –Audio playback issues are fixed. Audio sometimes did not play in View all songs\n–Missing spectrograms issue is fixed. Some users experienced missing spectrograms.\n–Download songs button (in View all songs) is fixed\n–Manually re-ordered columns now keep their custom order after refresh\nStuff in the works –An installer package for using Koe locally. Installing Koe on your local machine has several advantages. You will not be dependent on an internet connection, and tasks will be performed on your computer rather than on the Koe server, which can lead to improved performance. Koe can already be installed on your local computer (see https://github.com/fzyukio/koe#how-to-install-run-and-deploy-this-on-your-own), but we recognise that some users may find setup a little daunting. So we are planning an installer package that will make it simpler to get going with Koe on your own.\n"
},
{
	"uri": "http://koe-wiki.github.io/",
	"title": "Koe Documentation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://koe-wiki.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://koe-wiki.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]